{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0a1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8526869",
   "metadata": {},
   "source": [
    "# Scaled dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f677bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q,k,v, mask=None):\n",
    "    # q, k ,v = 30 x 8 x 200 x 64\n",
    "    d_k = q.size()[-1] # 64\n",
    "    scaled = torch.matmul(q,k.transpose(-1,-2)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n",
    "    if mask is not None:\n",
    "        scaled += mask # mask: (200 x 200) broadcasting add\n",
    "    attention = F.softmax(scaled, dim=-1) # 30 x 8 x 200 x 200\n",
    "    values = torch.matmul(attention,v) # 30 x 8 x 200 x 64\n",
    "    return values,attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17392ac7",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7d2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e17096",
   "metadata": {},
   "source": [
    "# Sentence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92265766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedding(nn.Module):\n",
    "    \"For a given sentence, create an embedding\"\n",
    "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    \n",
    "    def batch_tokenize(self, batch, start_token, end_token):\n",
    "\n",
    "        def tokenize(sentence, start_token, end_token):\n",
    "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
    "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
    "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_word_indicies)\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence_num in range(len(batch)):\n",
    "            tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized.to(get_device())\n",
    "    \n",
    "    def forward(self, x, start_token, end_token): # sentence\n",
    "        x = self.batch_tokenize(x, start_token, end_token)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86149722",
   "metadata": {},
   "source": [
    "# Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # 512\n",
    "        self.num_heads = num_heads # 8 \n",
    "        self.head_dim = d_model // num_heads # 64\n",
    "        self.qkv_layer = nn.Linear(d_model, 3 * d_model) # 512 x 1536\n",
    "        self.linear_layer = nn.Linear(d_model, d_model) # 512 x 512\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, d_model = x.size() # 30 x 200 x 512\n",
    "        qkv = self.qkv_layer(x) # 30 x 200 x (1536)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim) # 30 x 200 x 8 x 192\n",
    "        qkv = qkv.permute(0,2,1,3) # 30 x 8 x 200 x 192\n",
    "        q,k,v = qkv.chunk(3, dim=-1) # each 30 x 8 x 200 x 64\n",
    "        values, attention = scaled_dot_product(q, k, v) ## Values: 30 x 8 x 200 x 64, Attention: 30 x 8 x 200 x 200\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size,sequence_length, self.num_heads*self.head_dim) # 30 x 200 x 512\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530bd68",
   "metadata": {},
   "source": [
    "# MultiHead Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da826362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model , 2 * d_model) # 1024\n",
    "        self.q_layer = nn.Linear(d_model , d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, y, mask=None):\n",
    "        batch_size, sequence_length, d_model = x.size() # 30 x 200 x 512\n",
    "        kv = self.kv_layer(x) # 30 x 200 x 1024\n",
    "        q = self.q_layer(y) # 30 x 200 x 512\n",
    "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)  # 30 x 200 x 8 x 128\n",
    "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)  # 30 x 200 x 8 x 64\n",
    "        kv = kv.permute(0, 2, 1, 3) # 30 x 8 x 200 x 128\n",
    "        q = q.permute(0, 2, 1, 3) # 30 x 8 x 200 x 64\n",
    "        k, v = kv.chunk(2, dim=-1) # K: 30 x 8 x 200 x 64, v: 30 x 8 x 200 x 64\n",
    "        values, attention = scaled_dot_product(q, k, v, mask) #  30 x 8 x 200 x 64\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model) #  30 x 200 x 512\n",
    "        out = self.linear_layer(values)  #  30 x 200 x 512\n",
    "        return out  #  30 x 200 x 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072a4fc",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape # [512]\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape)) # [512] stddev of values\n",
    "        self.beta = nn.Parameter(torch.zeros(parameters_shape)) # [512] mean of values\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs): # 30 x 200 x 512\n",
    "        dims = [(-i + 1) for i in range(len(self.parameters_shape))] # [-1]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True) # 30 x 200 x 1\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True) # 30 x 200 x 1\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std # 30 x 200 x 512\n",
    "        out = self.gamma * y + self.beta # 30 x 200 x 512\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff5819",
   "metadata": {},
   "source": [
    "# Positionwise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden) # 512 x 2048\n",
    "        self.linear2 = nn.Linear(hidden, d_model) # 2048 x 512\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x): # 30 x 200 x 512\n",
    "        x = self.linear1(x) #  30 x 200 x 2048\n",
    "        x = self.relu(x) # 30 x 200 x 2048\n",
    "        x = self.dropout(x) # 30 x 200 x 2048\n",
    "        x = self.linear2(x) # 30 x 200 x 512\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8161c",
   "metadata": {},
   "source": [
    "# Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        residual_x = x # 30 x 200 x 512\n",
    "        x = self.attention(x, mask=None) # 30 x 200 x 512\n",
    "        x = self.dropout1(x) # 30 x 200 x 512\n",
    "        x = self.norm1(x + residual_x) # 30 x 200 x 512\n",
    "        residual_x = x # 30 x 200 x 512\n",
    "        x = self.ffn(x) # 30 x 200 x 512\n",
    "        x = self.dropout2(x) # 30 x 200 x 512\n",
    "        x = self.norm2(x + residual_x) # 30 x 200 x 512\n",
    "        return x \n",
    "\n",
    "class SequentialEncoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, self_attention_mask  = inputs\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length,\n",
    "                 language_to_index,\n",
    "                 START_TOKEN,\n",
    "                 END_TOKEN, \n",
    "                 PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
    "        x = self.sentence_embedding(x, start_token, end_token)\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81932dd",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self, x, y, decoder_mask):\n",
    "        _y = y\n",
    "        y = self.self_attention(y, mask = decoder_mask)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.norm1(y + _y)\n",
    "        \n",
    "        _y = y\n",
    "        y = self.encoder_decoder_attention(x, y, mask=None)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.norm2(y + _y)\n",
    "        \n",
    "        _y = y  \n",
    "        y = self.ffn(y) \n",
    "        y = self.dropout3(y) \n",
    "        y = self.norm3(y + _y) \n",
    "        return y # Every output essentially gives #30 x 200 x 512\n",
    "          \n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length,\n",
    "                 language_to_index,\n",
    "                 START_TOKEN,\n",
    "                 END_TOKEN, \n",
    "                 PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
    "        y = self.sentence_embedding(y, start_token, end_token)\n",
    "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789dc9e",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a352812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_model, \n",
    "                ffn_hidden, \n",
    "                num_heads, \n",
    "                drop_prob, \n",
    "                num_layers,\n",
    "                max_sequence_length, \n",
    "                gu_vocab_size,\n",
    "                english_to_index,\n",
    "                gujarati_to_index,\n",
    "                START_TOKEN, \n",
    "                END_TOKEN, \n",
    "                PADDING_TOKEN\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, gujarati_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.linear = nn.Linear(d_model, gu_vocab_size)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    def forward(self, \n",
    "                x, \n",
    "                y, \n",
    "                encoder_self_attention_mask=None, \n",
    "                decoder_self_attention_mask=None, \n",
    "                decoder_cross_attention_mask=None,\n",
    "                enc_start_token=False,\n",
    "                enc_end_token=False,\n",
    "                dec_start_token=False, # We should make this true\n",
    "                dec_end_token=False): # x, y are batch of sentences\n",
    "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
    "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32021c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae26d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3455e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b843903a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79315fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb19d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da9349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f0b76f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
