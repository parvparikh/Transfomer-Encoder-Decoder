{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8c9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
    "        scaled = scaled.permute(1, 0, 2, 3)\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE\n",
    "\n",
    "class SentenceEmbedding(nn.Module):\n",
    "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    \n",
    "    def batch_tokenize(self, batch, start_token, end_token):\n",
    "\n",
    "        def tokenize(sentence, start_token, end_token):\n",
    "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
    "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
    "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_word_indicies)\n",
    "#             print(f\"this is getting {self.vocab_size}\")\n",
    "        tokenized = []\n",
    "        for sentence_num in range(len(batch)):\n",
    "            tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized.to(get_device())\n",
    "    \n",
    "    def forward(self, x, start_token, end_token): # sentence\n",
    "        x = self.batch_tokenize(x, start_token, end_token)\n",
    "#         print(f\"WORKS TILL HERE {self}\")\n",
    "        x = self.embedding(x)\n",
    "#         print(f\"Idhar problem bhi nai he {self}\")\n",
    "        pos = self.position_encoder().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        batch_size, sequence_length, d_model = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, self_attention_mask):\n",
    "        residual_x = x.clone()\n",
    "        x = self.attention(x, mask=self_attention_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x\n",
    "    \n",
    "class SequentialEncoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, self_attention_mask  = inputs\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length,\n",
    "                 language_to_index,\n",
    "                 START_TOKEN,\n",
    "                 END_TOKEN, \n",
    "                 PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
    "        x = self.sentence_embedding(x, start_token, end_token)\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
    "        self.q_layer = nn.Linear(d_model , d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, y, mask):\n",
    "        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n",
    "        kv = self.kv_layer(x)\n",
    "        q = self.q_layer(y)\n",
    "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
    "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "        kv = kv.permute(0, 2, 1, 3)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
    "        _y = y.clone()\n",
    "        y = self.self_attention(y, mask=self_attention_mask)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.layer_norm1(y + _y)\n",
    "\n",
    "        _y = y.clone()\n",
    "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.layer_norm2(y + _y)\n",
    "\n",
    "        _y = y.clone()\n",
    "        y = self.ffn(y)\n",
    "        y = self.dropout3(y)\n",
    "        y = self.layer_norm3(y + _y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length,\n",
    "                 language_to_index,\n",
    "                 START_TOKEN,\n",
    "                 END_TOKEN, \n",
    "                 PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
    "#         print(f\"decoder ka sentense embedding {len(y)}\")\n",
    "        y = self.sentence_embedding(y, start_token, end_token)\n",
    "#         print(f\"decoder ka sentense embedding khatam {len(y)} \")\n",
    "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_model, \n",
    "                ffn_hidden, \n",
    "                num_heads, \n",
    "                drop_prob, \n",
    "                num_layers,\n",
    "                max_sequence_length, \n",
    "                kn_vocab_size,\n",
    "                english_to_index,\n",
    "                kannada_to_index,\n",
    "                START_TOKEN, \n",
    "                END_TOKEN, \n",
    "                PADDING_TOKEN\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, kannada_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.linear = nn.Linear(d_model, kn_vocab_size)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    def forward(self, \n",
    "                x, \n",
    "                y, \n",
    "                encoder_self_attention_mask=None, \n",
    "                decoder_self_attention_mask=None, \n",
    "                decoder_cross_attention_mask=None,\n",
    "                enc_start_token=False,\n",
    "                enc_end_token=False,\n",
    "                dec_start_token=False, # We should make this true\n",
    "                dec_end_token=False): # x, y are batch of sentences\n",
    "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
    "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ad7902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 96\n",
      "97th percentile length Gujarati: 163.02999999999884\n",
      "97th percentile length English: 182.0\n",
      "Number of sentences: 200000\n",
      "Number of valid sentences: 138070\n"
     ]
    }
   ],
   "source": [
    "english_file = './final_data/en-gu/train.en'\n",
    "gujarati_file = './final_data/en-gu/train.gu'\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = '<PAD>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "gujarati_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
    "                       '૦', '૧', '૨', '૩', '૪', '૫', '૬', '૭', '૮', '૯', ':', '<', '=', '>', '?',\n",
    "                       'અ', 'આ', 'ઇ', 'ઈ', 'ઉ', 'ઊ', 'ઋ', 'ૠ', 'ઍ', 'એ', 'ઐ', 'ઑ', 'ઓ', 'ઔ',\n",
    "                       'ક', 'ખ', 'ગ', 'ઘ',\n",
    "                       'ચ', 'છ', 'જ', 'ઝ',                        \n",
    "                       'ટ', 'ઠ', 'ડ', 'ઢ', 'ણ',                        \n",
    "                       'ત', 'થ', 'દ', 'ધ', 'ન',                        \n",
    "                       'પ', 'ફ', 'બ', 'ભ', 'મ',                        \n",
    "                       'ય', 'ર', 'લ', 'વ', 'શ', 'ષ', 'સ', 'હ', '઼', 'ા', 'િ', 'ી', 'ુ', 'ૂ', 'ૃ', 'ૄ', 'ૅ', 'ે', 'ૈ', 'ૉ', 'ો', 'ૌ', '્', 'ં', 'ઃ',\n",
    "                       PADDING_TOKEN, END_TOKEN]\n",
    "\n",
    "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                        ':', '<', '=', '>', '?', '@',\n",
    "                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', \n",
    "                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', \n",
    "                        'Y', 'Z',\n",
    "                        \"[\", \"/\", \"]\", \"^\", \"_\", \"`\", \n",
    "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
    "                        'y', 'z', \n",
    "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n",
    "\n",
    "\n",
    "index_to_gujarati = {k:v for k,v in enumerate(gujarati_vocabulary)}\n",
    "gujarati_to_index = {v:k for k,v in enumerate(gujarati_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}\n",
    "\n",
    "print(len(gujarati_to_index),len(english_to_index))\n",
    "\n",
    "with open(english_file, 'r') as file:\n",
    "    english_sentences = file.readlines()\n",
    "with open(gujarati_file, 'r') as file:\n",
    "    gujarati_sentences = file.readlines()\n",
    "\n",
    "TOTAL_SENTENCES = 200000 #129862\n",
    "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
    "gujarati_sentences = gujarati_sentences[:TOTAL_SENTENCES]\n",
    "english_sentences = [sentence.rstrip('\\n') for sentence in english_sentences]\n",
    "gujarati_sentences = [sentence.rstrip('\\n') for sentence in gujarati_sentences]\n",
    "\n",
    "\n",
    "PERCENTILE = 97\n",
    "# 97% of Gujarati Sentences have less than 163 charcaters \n",
    "print( f\"{PERCENTILE}th percentile length Gujarati: {np.percentile([len(x) for x in gujarati_sentences], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n",
    "\n",
    "\n",
    "max_sequence_length = 200\n",
    "def is_valid_tokens(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_valid_length(sentence, max_sequence_length):\n",
    "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
    "\n",
    "valid_sentence_indicies = []\n",
    "for index in range(len(gujarati_sentences)):\n",
    "    gujarati_sentence, english_sentence = gujarati_sentences[index], english_sentences[index]\n",
    "    if is_valid_length(gujarati_sentence, max_sequence_length) \\\n",
    "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
    "      and is_valid_tokens(gujarati_sentence, gujarati_vocabulary):\n",
    "        valid_sentence_indicies.append(index)\n",
    "\n",
    "print(f\"Number of sentences: {len(gujarati_sentences)}\")\n",
    "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")\n",
    "\n",
    "gujarati_sentences = [gujarati_sentences[i] for i in valid_sentence_indicies]\n",
    "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4641ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "d_model = 512\n",
    "batch_size = 30\n",
    "ffn_hidden = 2048\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "num_layers = 1\n",
    "max_sequence_length = 200\n",
    "gu_vocab_size = len(gujarati_vocabulary)\n",
    "\n",
    "transformer = Transformer(d_model, \n",
    "                          ffn_hidden,\n",
    "                          num_heads, \n",
    "                          drop_prob, \n",
    "                          num_layers, \n",
    "                          max_sequence_length,\n",
    "                          gu_vocab_size,\n",
    "                          english_to_index,\n",
    "                          gujarati_to_index,\n",
    "                          START_TOKEN, \n",
    "                          END_TOKEN, \n",
    "                          PADDING_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5df7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(96, 512)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialEncoder(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNormalization()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(96, 512)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNormalization()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
       "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNormalization()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=96, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3100d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, english_sentences, gujarati_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.gujarati_sentences = gujarati_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.gujarati_sentences[idx]\n",
    "     \n",
    "dataset = TextDataset(english_sentences, gujarati_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29c1590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138070\n",
      "('Jaish-e-Mohammed (JeM) control rooms were also reportedly destroyed.', 'જૈશ-એ-મોહમ્મદના કંટ્રોલ રૂમ પણ નષ્ટ થઈ ગયા છે.')\n"
     ]
    }
   ],
   "source": [
    "dataset = TextDataset(english_sentences, gujarati_sentences)\n",
    "print(len(dataset))\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9addc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2519957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('But many have tried to supply missing details.', 'Jaish-e-Mohammed (JeM) control rooms were also reportedly destroyed.', 'He declined to speak further on the issue.', 'He was also the national vice-president of the All India Bharatiya Janata Yuva Morcha, the youth wing of the BJP.', 'Congress is not bothered about the depletion of natural resources here, Modi said.', 'Lets try to understand them.', 'This decision has been taken at a meeting.', 'But just a moment!', 'Brad Pitt has worked in several popular films.', 'But the Congress was sad.', 'PM interacts with Sportspersons', 'All are undergoing treatment in a hospital.', 'Narendra Modi today, appealed for support from the citizens on following seven points:', '( Compare Deuteronomy 32: 5.)', 'And how does one do that?', 'a sink with some dirty dishes in it', 'This issue will be discussed.', 'death-body.', 'She was going to London by British Airways.', 'However, the motive behind this murder was not known.', 'However, Pakistan has refused to accept the dead bodies.', 'He had sought for political asylum in the US.', 'Thanks for your patience!', 'A young woman has been killed.', 'However there was no casualty.', 'New features coming on WhatsApp.', 'This is being done as a precautionary measure, he added.', 'This is truly made in India.', 'This was announced by Union Minister for Information Broadcasting, Shri Prakash Javadekar in New Delhi today.', 'MG Motors'), ('ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.', 'જૈશ-એ-મોહમ્મદના કંટ્રોલ રૂમ પણ નષ્ટ થઈ ગયા છે.', 'તેમણે આ અંગે વધુ વાત કરવાનો ઇનકાર કર્યો.', 'સાથોસાથ ભારતીય જનતા પાર્ટીના યુવા મોરચાના રાષ્ટ્રીય અધ્યક્ષ પદે તેની તાજપોશી થાય એ માટે પણ દાવ ખેલ્યો હતો.', '\"આ રેલીમાં મોદીએ કહ્યું, \"\"અહીંયા કુદરતી સ્ત્રોતોમાં થઇ રહેલા ઘટાડાની કોંગ્રેસને કોઇ પરવા .\"', 'માતાનો તેમને વિગતવાર સમજવા માટે પ્રયાસ કરીએ.', 'આ બેઠક બાદ આ નિર્ણય લેવામાં આવ્યો છે.', 'ફક્ત એ એકાદ ક્ષણ જ !', 'હોલિવુડ સ્ટાર બ્રાડ પીટ કેટલીક યાદગાર ફિલ્મોમાં કામ કરી ચુક્યો છે.', 'પણ કોંગ્રેસ અંદરખાને ડરેલું હતું.', 'પ્રધાનમંત્રીએ રમતવીરો સાથે વાત કરી', 'બધાનો કોરોના હોસ્પિટલમાં ઈલાજ ચાલી રહ્યો છે.', 'માનનીય પ્રધાનમંત્રી શ્રી નરેન્દ્ર મોદીએ આજે રાષ્ટ્રજોગ સંદેશમાં નીચે દર્શાવેલા સાત મુદ્દા પર સહકાર આપવા માટે તમામ નાગરિકોને અપીલ કરી હતી:', '( વધુ માહિતી: પુનર્નિયમ ૩૨: ૫.)', 'અને તમે તે પ્રકારના કામ કેવી રીતે કરશો?', 'તેમાં કેટલાક ગંદા વાનગીઓ સાથે સિંક', 'મામલો ચર્ચાશે.', 'અહેડ - મૃત્યુ.', 'તે લંડન જવા માટે ફ્લાઈટ લેવા જઇ રહી હતી.', 'જોકે, હત્યાનું કારણ જાણ શકાયું નથી.', 'પરંતુ પાકિસ્તાને હંમેશા મૃતદેહ સ્વીકારવાનો ઈન્કાર કર્યો છે.', 'તેમણે યુએસમાં રાજકીય શરણની માગ કરી હતી.', 'તમારી ધીરજ માટે આભાર,', 'એક યુવકે મહિલાને ઢોર માર માર્યો.', 'જોકે તેમાં કોઈ જાનહાનિ નથી થઈ.', 'વોટ્સએપ પર આવશે નવા ફીચર્સ', \"આથી એક અગમચેતીનું પગલું લેવાયું છે' તેમ તેમણે ઉમેર્યું હતું.\", 'તેને ભારતમાં જ તૈયાર કરવામાં આવી છે.', 'કેન્દ્રીય માહિતી અને પ્રસારણ મંત્રી શ્રી પ્રકાશ જાવડેકરે આજે નવી દિલ્હીમાં તેની જાહેરાત કરી હતી.', 'એમજી મોટર્સ')]\n",
      "[('Take care of health.', 'Added features', 'The police then swung into action.', 'to cut it.', 'a locomotive train resting on some tracks next to a building', 'What can help us to build strong faith?', 'So what do you want?', \"The pictures from Neha Kakkar and Rohanpreet's wedding ceremony are going viral on social media.\", 'Bollywoods highest earning celebs', 'Practice 1.', 'The highly fragmented pre-owned car market is divided into four categoriesthe organised, semi-organised, unorganised and consumer-to-consumer market.', 'There is a lot of unauthorised construction there.', 'I recall that one of them kept contacting me to see how I was.', 'Exodus simply states that he grew up with his parents, which can signify reaching any age.', 'The Integral Coach Factory, Chennai manufactures the train.', 'It was decided to hold the virtual meetings of technical officers of member countries every Monday via Skype and nine meetings have so far been held.', 'What if he doesnt love me?', 'What personal achievements could we claim that could ever compare with Gods accomplishments?', 'When that prayer is fully answered, conditions that foster hatred will exist no more. Situations that exploit it will have been eliminated.', 'Action according to law has been taken.', 'How can this be solved?', 'Both Chennai and Mumbai have won the title thrice.', 'The body of the child has been sent for postmortem.', 'Congress President Rahul Gandhi and many other senior party leaders also participated in the road show.', 'The animosity between the workers of the BJP and Trinamool Congress continues.', \"It's an old Indian tradition.\", 'As mentioned earlier, the nation of Israel exclusively had the prospect of supplying members to make up a kingdom of priests and a holy nation.', 'Pregnancy complications', 'Following are the results :', 'I did not react.'), ('સ્વાસ્થ્ય નું રાખો ખ્યાલ.', 'ઉમેરાયેલ લક્ષણો', 'ત્યારબાદ પોલીસ એક્શનમાં આવી ગઇ હતી.', 'તેની ઝીણી કટકી કરવી.', 'મકાનની બાજુમાં કેટલાક ટ્રેક્સ પર આરામ કરતા લોકોમોટિવ ટ્રેન', 'આપણી શ્રદ્ધા કઈ રીતે મજબૂત કરી શકીએ?', 'તો આપને કઈ વાતનો ઇંતેજાર છે ?', 'નેહા અને રોહનના લગ્નના વીડિયો સોશ્યલ મીડિયામાં વાયરલ થઇ રહ્યા છે.', 'બોલિવૂડ સેલેબ્સની કરોડોની કમાણી', 'પ્રેક્ટિસ.', 'અત્યંત વિકેન્દ્રીત પ્રી-ઓન્ડ કાર માર્કેટ મુખ્યત્વે ચાર શ્રેણીઓમાં વહેંચાયેલું છે, જેમાં સંગઠિત, અર્ધ-સંગઠિત, બિનસંગઠિત તથા ગ્રાહક- ગ્રાહક વચ્ચેના સોદાનો સમાવેશ થાય છે.', 'તે જગ્યાએ અનેક મંજૂરી વગરના બાંધકામો આવેલા છે.', 'પણ મને ઘણી જ ખુશી થઈ કે સાક્ષીઓ મને ભૂલી ગયા ન હતા.', 'પરંતુ અમુક લોકોનું માનવું છે કે તે ત્રણેક વર્ષે ધાવણ છોડે ત્યાં સુધી અથવા થોડો લાંબો સમય સુધી રહ્યા હોય શકે.', 'ચેન્નાઈ સ્થિત ઈન્ટ્રિગલ કોચ ફેક્ટર આ ટ્રેનનું નિર્માણ કરે .', 'ત્યારે સભ્યો દેશોના ટેકનિકલ અધિકારીઓની દર સોમવારે સ્કાઇપે દ્વારા વર્ચ્યુઅલ બેઠકો યોજવાનું નક્કી કરવામાં આવ્યું હતું અને અત્યાર સુધીમાં આવી નવ બેઠકોનું આયોજન કરવામાં આવ્યું છે.', '\"\"\"જો હું પ્રેમ ન કરું\"\"\"', 'શું આપણે આપણા કોઈ પણ કામની સરખામણી દેવના કામો સાથે કરી શકીએ છીએ?', 'પરંતુ ત્યારે એવું નહિ હોય, એના બદલે તેઓને પૂરતી માહિતી હશે, તેઓ સત્ય જાણનારા અને ન્યાયી હશે.', 'કરી કાયદેસરની કાર્યવાહી કરવામાં આવેલ છે.', 'આનો ઉકેલ શી રીતે આવી શકે?', 'મુંબઈ અને ચેન્નાઈએ ત્રણ-ત્રણ વખત આઈપીએલ ટાઈટલ જીત્યા છે.', 'સગીરાનો મૃતદેહ પોસ્ટમોર્ટમ માટે ખસેડાયો છે.', 'કોંગ્રેસ અધ્યક્ષ રાહુલ ગાંધી દ્વારા આયોજીત ઈફ્તાર પાર્ટીમાં ઘણા દિગ્ગજ નેતાઓ હાજર રહ્યા હતા.', 'ભાજપ અને તૃણમુલ કોંગ્રેસના કાર્યકરો વચ્ચેની હિંસક ઝડપની ઘટનાઓ સતત વધી રહી છે.', 'આ પ્રાચીન ભારતીય પદ્ધતિ છે.', 'આપણે જોઈ ગયા તેમ, ફક્ત ઈસ્રાએલી પ્રજા પાસે એક મોટો લહાવો હતો.', 'પ્રેગનન્સી દરમીયાન રહેતી તકલીફો', 'નીચેના પરિણામો દર્શાવે છે:', 'મેં કશી પણ પ્રતિક્રિયા ન આપી.')]\n",
      "[('Is this civilization?', 'You can trust me on that.', 'The President urged students of Gujarat University to take advantage of these facilities', 'The Egyptian ruler defiantly refuses.', 'You need not worry.', 'The security forces have launched a search operation in the area.', 'Have a look at that.', 'How can we be sure that it was God who inspired the Bible writers?', 'A city bus is parked on the curb waiting for people', 'Lets know about this.', 'Coronavirus has taken a toll over several countries of the world.', 'The family registered a missing complaint with the police.', 'It has become a habit with some people.', 'In some cases, it can even lead to trials persecution and suffering.', 'Today, society promotes the view that people would be happy if only they were rich enough.', \"Interestingly Deepika's first look from the film was unveiled a few days back.\", 'Lok Sabha and assembly elections are being held simultaneously in the state.', 'Hence we should be more vigilant.', 'Petrol, diesel prices lowered', 'The death toll might go up further.', 'He was posted in the ICU.', 'The film is based on the story of Arunachalam Muruganantham.', 'What finally happened?', 'Holy month of Shravan begins', 'the rivers have been inundated.', 'PM Narendra Modi warmly received US President Donald Trump.', 'This can cause acidity.', 'It cannot take any decision on its own.', 'Both the Prime Ministers were participating through Video Conference', 'No new case has been reported.'), ('શું આ છે સભ્યતા ?', 'તમે મારા ઉપર વિશ્વાસ મૂકી શકો છો.', 'રાષ્ટ્રપતિએ ગુજરાત યુનિવર્સિટીના વિદ્યાર્થીઓને આ સવલતોનો લાભ લેવાનો અનુરોધ કર્યો હતો', 'પરંતુ, ફારૂન સીધેસીધી ના પાડી દે છે.', 'તનેય વાંધો ન હોવો જોઈએ.', 'સુરક્ષાકર્મીઓએ વિસ્તારની ઘેરાબંધી કરી તપાસ શરૂ કરી દીધી છે.', 'તેના પર એક નજર નાંખો.', 'ઈશ્વરે બાઇબલ લેખકોને પ્રેરણા આપી, એવી ખાતરી કઈ રીતે રાખી શકીએ?', 'શહેરની બસ લોકોની રાહ જોઈ રહેલા અંકુશ પર બાંધી છે', 'ચાલો આપણે આ વિશે જાણીએ.', 'કોરોના વાયરસના કારણે વિશ્વનાં ઘણા દેશોમાં તારાજી સર્જાઈ છે.', 'પરિવારે પોલીસ મથકે ગુમ થયાની ફરિયાદ નોંધાવી હતી', 'કેટલાંક લોકોમાં તે લગાવ બની ચૂક્યો છે.', 'એનો એ અર્થ થાય છે કે, આપણને યહોવાહની સેવા કરવા અને તેમની ઇચ્છા પ્રમાણે જીવવા ઉત્તેજન આપવામાં આવે છે.', 'આજે સમાજમાં એવું પ્રોત્સાહન આપવામાં આવે છે કે જો વ્યક્તિ ખૂબ ધનવાન થશે તો જ સુખી થશે.', 'કેટલાક દિવસ પહેલા જ ફિલ્મમાં દીપિકાનો ફર્સ્ટ લુક જાહેર કરાયો હતો.', 'દેશમાં લોકસભા અને વિધાનસભા ચૂંટણીઓ એક સાથે યોજવા માટે છેલ્લાં કટલાંક સમયથી વિચારણા ચાલી રહી છે.', 'માટે આપણે પહેલેથી પણ વધુ સતર્કતા દાખવવી પડશે.', 'પેટ્રોલ અને ડીઝલના ભાવમાં થયો ઘટાડો', 'મૃત્યુઆંક હજી વધી શકે તેમ છે.', 'તેને આઈસીયુમાં ભરતી કરવામાં આવ્યા હતા.', 'આ ફિલ્મ અરુનાચલમ મુરુગનાથમની સ્ટોરી પર આધારિત છે.', 'આખરે શું થયું હતું?', 'પવિત્ર શ્રાવણ માસની શરૂઆત', 'નદીઓ ગાંડીતૂર થઈ છે.', 'પ્રધાનમંત્રી નરેન્દ્ર મોદીએ ,નમસ્તે ટ્રમ્પ કહીને ,ડોનાલ્ડ ટ્રમ્પનું સ્વાગત કર્યું.', 'તેનાથી એસિડિટી થઈ શકે છે.', 'એ પોતાની જાતે જ સમજીને-વિચારીને નિર્ણય નહોતો લઈ શકતો.', 'બંને પ્રધાનમંત્રીઓએ વીડિયો કોન્ફરન્સ મારફતે આ કાર્યક્રમમાં ભાગ લીધો હતો', 'અહીં કોઈ પણ નવો કેસ સામે આવી રહ્યો નથી.')]\n",
      "[('Who is Suman Rao?', 'Who will help?', 'The photo has gone viral on social media.', '\"\"\"The house was surrounded.\"', 'Key recommendations:', 'South Mumbai has a number of major market places clothes, diamonds, jewellery, pharmaceuticals, electronics, foreign merchandise and a number of corporate houses.', 'There is however, some truth in it.', 'But the story doesnt end here.', 'Jehovah promises real protection against anything Satan can do.', 'bond funds', 'She then informed the matter to her husband.', 'Farmers Day', '( b) Of what outcome can we be confident?', 'Durga Puja celebrates the victory of the goddess Durga over the demon king Mahishasura.', 'This has been claimed in a research.', 'Supporters of Congress candidate Prashant Patel take out a nomination filing rally in Vadodara.', 'It was truly a memorable evening.', 'Why have interest rates come down', 'Theres a lot of time left.', 'The entire power is with you.', 'A group of people standing around a green train car.', 'And then it started raining.', '4: 12, 13. Why can Christians rejoice when they are persecuted?', 'He had many ups and downs in his life.', 'We have no such confusion.', 'Stay calm... it will happen only when we want it.', 'what are trps?', 'an office job.', 'They are washed and cleaned thoroughly in normal temperature water.', 'Daily, Jehovahs people were being arrested and brought before the courts.'), ('કોણ છે સુમન રાવ?', 'કોણ આધાર કરશે?', 'આ ફોટો સોશિયલ મીડિયામાં વાયરલ થયો છે.', '\"\"\"ઘર પોતે પિચીંગ હતું.\"', 'મહત્વપૂર્ણ ટિપ્સ:', 'દક્ષિણ મુંબઈમાં અનેક માર્કેટ છે. જેમાં કપડાં, હીરા, જ્વેલરી, ફાર્માસ્યુટિકલ, ઇલેક્ટ્રૉનિક્સ, વિદેશી કંપનીઓ તથા અનેક કૉપોર્રેટ કંપનીઓ આવી છે.', 'જોકે અહીં કેટલાક સત્ય છે.', 'પરંતુ ઇતિહાસ અહીં સમાપ્ત થતું નથી.', 'શેતાનની કોઈ પણ કુયુક્તિઓ વિરુદ્ધ રક્ષણ કરવાનું યહોવાહે વચન આપ્યું છે.', 'બોન્ડ ફંડ્સ', 'અને આ મામલે પતિને જાણ કરી હતી.', 'ગ્રામજનોની ડે.', 'તેઓને ઈશ્વરનો જરાય ડર ન હતો.', 'મહિષાસુર નામના અસુરનો વધ કરનાર મા દુર્ગાના વિજયની સ્મૃતિમાં દુર્ગાપૂજાની ઉજવણી કરવામાં આવે છે.', 'એક અભ્યાસમાં દાવો કરવામાં આવ્યો છે.', 'વડોદરાના કોંગ્રેસ ઉમેદવાર પ્રશાંત પટેલે પ્રચારના શ્રીગણેશ કર્યા', 'સાંજે ખરેખર યાદગાર હતી.', 'વ્યાજદર કેમ ઘટાડયા ?', 'એટલો સમય બચે છે.', 'સર્વ શક્તિ તમારી અંદર છે.', 'ગ્રીન ટ્રેન કારની આસપાસ ઊભેલા લોકોનો સમૂહ', 'અને પછી શરૂ થયો ધોધમાર વરસાદ !', 'આપણો વિરોધ કે સતાવણી થાય તો શા માટે હરખાવું જોઈએ?', 'તેમના જીવન માં, ત્યાં ઘણા અપ્સ એન્ડ ડાઉન્સ હતી.', 'આપણે ત્યાં આવી સંકુચિતતા નથી.', 'શાંતિ રાખો આ ત્યારે જ થશે જ્યારે અમારી ઈચ્છા હશે.', 'ટેરિયર્સ શું છે?', 'ઓફિસ વર્કલોડ.', 'તેઓ સંપૂર્ણપણે ધોવાઇ અને સામાન્ય પાણીમાં બાફેલી કરવામાં આવી હતી.', 'દરરોજ યહોવાહના લોકોની ધરપકડ કરવામાં આવતી અને તેઓને અદાલતમાં લાવવામાં આવતા.')]\n",
      "[('Earlier, many BJP leaders have made similar statements.', 'Germany is placed third.', 'Add garlic and stir.', 'The fee can be paid online by using either VISA or MASTER or MAESTRO Credit/Debit Card/Rupay Card/ Net Banking.', 'What family had said', 'Hence there is great need to stop it.', 'Warning message box with yes/ no buttons', 'The film did particularly well in north India and Punjab.', 'But why did this happen so instantly?', 'First week', 'He got terribly afraid.', 'A police complaint was registered in the matter.', 'BJP committed to building Ram temple in Ayodhya: Amit Shah', 'The Committee will have powers on conditions within reasonable limits, on a case by case basis, during the first two years of implementation of the scheme.', 'Read the full report', 'My sympathies are with his family.', 'We are trying to do that.', 'Regarding this remarkable cycle, the Bible says: All the winter torrents are going forth to the sea, yet the sea itself is not full.', 'Happiness will increase in your family.', 'People have reacted crazily.', 'Since his rebellion, Satan has proved to be Jehovahs greatest enemy, and he certainly has not been a friend of mankind.', 'Amazon.in', 'Myth: Sugar causes diabetes', \"Don't disclose your personal information over the phone.\", 'It invokes love.', 'Its not you, its them', 'smartphone market in India', 'If you lend to those from whom you hope to receive, what credit is that to you? Even sinners lend to sinners, to receive back as much.', 'Take care of food and drink.', 'Debit Card'), ('જોકે આ પહેલા બીજેપીના અનેક મોટા નેતાઓ પણ અપમાનજનક ટિપ્પણી કરી ચૂક્યા છે.', 'આ બાબતે જર્મની બીજા સ્થાન પર છે.', 'લસણ ઉમેરો અને મિશ્રણ.', 'પે રોકડ અથવા કાર્ડ યુરોકાર્ડ, માસ્ટ્રો, વિઝા, માસ્ટરકાર્ડ દ્વારા હોઈ શકે છે.', 'શું કહ્યું પરિવારજનોએ', 'તેથી તેના પર રોક લગાવવાની સખ્ય જરૂરત છે.', 'હા/ ના બટનો સાથે ચેતવણી સંદેશ બોક્સ', 'ફિલ્મે ઉત્તર અને પૂર્વ ભારતમાં ખાસ સારુ પરફોર્મ નથી કર્યું.', 'પણ અચાનક આ રીતે ઘોડાપૂર કેમ આવ્યાં?', 'પ્રથમ અઠવાડિયું', 'તે ડરતો-ડરતો આવ્યો.', 'જે અંગે પોલીસ ચોપડે ફરિયાદ નોંધાઈ હતી.', 'અયોધ્યામાં હવે બનશે ગગનચુંબી રામ મંદિર: અમિત શાહ', 'આ સમિતિને વાજબી મર્યાદાની અંદર, યોજનાના અમલના પ્રથમ બે વર્ષમાં કેસવાર શરતો મૂકવાની સત્તા રહેશે.', 'વાંચો સંપૂર્ણ રિપોર્ટ', 'મારી સંવેદના તેમના પરિવાર સાથે છે.', 'અમે આ માટે પ્રયાસ કરી રહ્યા છીએ.', 'મોટા ભાગનું પાણી દરિયા, સરોવર, નદીઓ, હિમનદીઓ, બરફથી છવાયેલો રહેલો ધ્રૃવ વિસ્તાર અને જમીનમાં એકઠું થાય છે.', 'પરિવારમાં સુખશાંતિ વધશે.', 'લોકોએ આવી ચોંકાવનારી પ્રતિક્રિયા આપી છે.', 'બાઇબલમાં કરવામાં આવેલ શેતાનના વર્ણન પરથી જોઈ શકાય કે તે કેટલો ભ્રષ્ટ છે.', 'એમેઝોન. કોમ', 'ભ્રમ: ખાંડ ખાવાથી ડાયાબિટીસ થાય છે', 'ક્યારેય તમારી વિગત ફોન પર કોઈને ન આપશો.', 'એ પરતંત્રતા પ્રેમને પ્રગટાવે છે.', 'તે તમે નથી-તે છે', 'ભારતમાં સ્માર્ટફોનનું બજાર નિર્ણાયક તબક્કે', 'હંમેશા જેઓની પાસેથી તમે પાછું લેવાની આશા રાખો, તેઓને જ તમે ઊછીનું આપો, તો તેમાં તમારી મહેરબાની શાની? ના! પાપીઓ પણ પાછું લેવા માટે પાપીઓને ઊછીનું આપે છે!', 'ખાવા-પીવા ઉપર ધ્યાન રાખવું.', 'ડીજિટલ કાર્ડ')]\n"
     ]
    }
   ],
   "source": [
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f2a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=gujarati_to_index[PADDING_TOKEN],\n",
    "                                reduction='none')\n",
    "\n",
    "# When computing the loss, we are ignoring cases when the label is the padding token\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0133e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, gu_batch):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "        eng_sentence_length, gu_sentence_length = len(eng_batch[idx]), len(gu_batch[idx])\n",
    "        eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
    "        gu_chars_to_padding_mask = np.arange(gu_sentence_length + 1, max_sequence_length)\n",
    "        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_self_attention[idx, :, gu_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_self_attention[idx, gu_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_cross_attention[idx, gu_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1915904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Iteration 0 : 5.609126091003418\n",
      "English: But many have tried to supply missing details.\n",
      "gujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\n",
      "gujarati Prediction: અભઔઔઔ૯૯અઅઅઅઔઔઔઔઔઔઅઔઔઔઅભઅહુહુઔ્હભઔભભઔુુભ=ભ/તણએલભ,ણ,\">્ભભઊભ૬૬૬ભભઊભભભભ?હટણહહલ?ઋ>ણ>ઊટ૭ભઆૌતભભ?લભલૌૌ>ભઔભભઋભઋઠ=તભ૧૧૧ભભઠ?ભભભ?ભ???ઈઠભ??૭હઋભ???ત૭?૭૭?/??૭ભ#=#ભભભ#ભભભભભ૭ભભ,૭૨૭ઔૄભષઋ૧ભઅભભષભ઼઼ભ>઼ભઋ>#ભ૧##ભત##઼#઼ભ૭૭૭ભ\n",
      "Evaluation translation (should we go to the mall?) : ('                                         તતતત   ાતત્    ત ૬           હહહ            ત      ટટટટટટ   ક   ત૧          ????ત ત ત  ??????૭૭????    #    ####ક૭૭૭૭્કકકક ્્્તતતષષ તતતતતતત    કક ત       તતતકક',)\n",
      "-------------------------------------------\n",
      "Iteration 100 : 3.441028356552124\n",
      "English: a man sitting on a bench looking down\n",
      "gujarati Translation: નીચે જોઈ બેન્ચ પર બેઠા માણસ\n",
      "gujarati Prediction: ેમરા              ી  ા       ા                 ા   ા      ાા    ્                ર             ાાા  ે     ા ા            ા   ા  ા      ે ે      ા   ા ાા્ી્ીેકે  ્ા ાા  ેેમી        ા       ી         ા \n",
      "Evaluation translation (should we go to the mall?) : ('તા                              <END>',)\n",
      "-------------------------------------------\n",
      "Iteration 200 : 3.2150299549102783\n",
      "English: Don't you ever get angry?\n",
      "gujarati Translation: ત્યારે શું તમને ગુસ્સો નહીં આવે?\n",
      "gujarati Prediction: આેે        મ    ા   ર  ે ર  છેી \n",
      "Evaluation translation (should we go to the mall?) : ('તે ર સ પા રા પ પા ર ર ર ર છે છે.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 300 : 2.864013671875\n",
      "English: Add sugar and salt.\n",
      "gujarati Translation: ખાંડ અને મીઠું સ્વાદ ઉમેરવામાં આવે છે.\n",
      "gujarati Prediction: તાન ્વ ે ન     છારા રછુા.ાાંા .છ ા.છે.\n",
      "Evaluation translation (should we go to the mall?) : ('તે પા પા પા પા છે.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 400 : 2.7561697959899902\n",
      "English: Therefore, we must observe the following rules:\n",
      "gujarati Translation: આમ નિયમો અનુસરો કરવા માટે જરૂરી છે:\n",
      "gujarati Prediction: આેાકીસાા સર ંા  સા્ા સા ે પેાર  છે \n",
      "Evaluation translation (should we go to the mall?) : ('તે કા સા પા પા છે છે છે છે.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 500 : 2.8355610370635986\n",
      "English: Players released: Hanuma Vihari, Jalaj Saxena, Manjot Kalra, Ankush Bains, Nathu Singh, Bandaru Ayappa, Chris Morris, Colin Ingram, Colin Munro\n",
      "gujarati Translation: રિલીઝ ખેલાડી: હનુમા વિહારી, જલજ સક્સેના, મનજોત કાલરા, અંકુશ બૈંસ, નાથુ સિંહ, બંડારૂ અયપ્પા, ક્રિસ મોરિસ, કોલિન ઇનગ્રામ, કોલિન મુનરો\n",
      "gujarati Prediction: આ્તા  તા ાંા  સાા ા સાક્ં   કાા ક્ાયા    સાા્ ાકાંા્  પ  ાંાપાય ્ પ  ાંક વ ા ક્ ાંા વરારર્  પારી ાઆા ા   આા ે ેછ ા્ય  ા તા ેયેકા ા     ય       ર     યરરરરરર  રર      ર ય યમ  ર ય રયય  ર  ર રર યય યયયરયર\n",
      "Evaluation translation (should we go to the mall?) : ('આ કારા પ્યા પ્યાં છે છે.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 600 : 2.6766724586486816\n",
      "English: And empty intersection with several cars parked along the street.\n",
      "gujarati Translation: અને શેરીમાં પાર્ક કરેલી કેટલીક કાર સાથે ખાલી આંતરછેદ.\n",
      "gujarati Prediction: તરા તા ા ા  સરરાર અાા ા સર ેા ાકાંીકા ી છાંા પત ાીે ા\n",
      "Evaluation translation (should we go to the mall?) : ('તે કાં સાં કાં છે.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 700 : 2.6965010166168213\n",
      "English: Actor and director\n",
      "gujarati Translation: ડિરેક્ટર અને અભિનેતા\n",
      "gujarati Prediction: તેલ્  રે કને\n",
      "Evaluation translation (should we go to the mall?) : ('તે તે પરે કરી છે.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 800 : 2.738068103790283\n",
      "English: The Chief Minister has earmarked Rs.\n",
      "gujarati Translation: મુખ્યમંત્રીએ સુરત મહાનગરમાં પ્રધાનમંત્રી આવાસ યોજનાના રૂ.\n",
      "gujarati Prediction: આાંાય ા ાર   પ્ં  પાતરી  ાં છરયીારીા ાય  છવાં છો.ીો.ાંછ વ\n",
      "Evaluation translation (should we go to the mall?) : ('તે પ્યાં પર્યાં કર્યાં છે.<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_486528/2167741354.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                      \u001b[0menc_end_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                      \u001b[0mdec_start_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                      dec_end_token=True)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#         print(\"ye nahi ho raha\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgu_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_486528/34998793.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[0m\n\u001b[1;32m    302\u001b[0m                 dec_end_token=False): # x, y are batch of sentences\n\u001b[1;32m    303\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_start_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_end_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_self_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_cross_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_start_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_end_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_486528/34998793.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;31m#         print(f\"decoder ka sentense embedding khatam {len(y)} \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_486528/34998793.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_486528/34998793.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, self_attention_mask, cross_attention_mask)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_486528/34998793.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.train()\n",
    "transformer.to(device)\n",
    "total_loss = 0\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    iterator = iter(train_loader)\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        transformer.train()\n",
    "        eng_batch, gu_batch = batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, gu_batch)\n",
    "        optim.zero_grad()\n",
    "#         print(\"ye to ho gya\")\n",
    "        gu_predictions = transformer(eng_batch,\n",
    "                                     gu_batch,\n",
    "                                     encoder_self_attention_mask.to(device), \n",
    "                                     decoder_self_attention_mask.to(device), \n",
    "                                     decoder_cross_attention_mask.to(device),\n",
    "                                     enc_start_token=False,\n",
    "                                     enc_end_token=False,\n",
    "                                     dec_start_token=True,\n",
    "                                     dec_end_token=True)\n",
    "#         print(\"ye nahi ho raha\")\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(gu_batch, start_token=False, end_token=True)\n",
    "        loss = criterian(\n",
    "            gu_predictions.view(-1, gu_vocab_size).to(device),\n",
    "            labels.view(-1).to(device)\n",
    "        ).to(device)\n",
    "        valid_indicies = torch.where(labels.view(-1) == gujarati_to_index[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #train_losses.append(loss.item())\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
    "            print(f\"English: {eng_batch[0]}\")\n",
    "            print(f\"gujarati Translation: {gu_batch[0]}\")\n",
    "            gu_sentence_predicted = torch.argmax(gu_predictions[0], axis=1)\n",
    "            predicted_sentence = \"\"\n",
    "            for idx in gu_sentence_predicted:\n",
    "                if idx == gujarati_to_index[END_TOKEN]:\n",
    "                      break\n",
    "                predicted_sentence += index_to_gujarati[idx.item()]\n",
    "            print(f\"gujarati Prediction: {predicted_sentence}\")\n",
    "\n",
    "\n",
    "            transformer.eval()\n",
    "            gu_sentence = (\"\",)\n",
    "            eng_sentence = (\"should we go to the mall?\",)\n",
    "            for word_counter in range(max_sequence_length):\n",
    "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, gu_sentence)\n",
    "                predictions = transformer(eng_sentence,\n",
    "                                          gu_sentence,\n",
    "                                          encoder_self_attention_mask.to(device), \n",
    "                                          decoder_self_attention_mask.to(device), \n",
    "                                          decoder_cross_attention_mask.to(device),\n",
    "                                          enc_start_token=False,\n",
    "                                          enc_end_token=False,\n",
    "                                          dec_start_token=True,\n",
    "                                          dec_end_token=False)\n",
    "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
    "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "                next_token = index_to_gujarati[next_token_index]\n",
    "                gu_sentence = (gu_sentence[0] + next_token, )\n",
    "                if next_token == END_TOKEN:\n",
    "                    break\n",
    "            \n",
    "            print(f\"Evaluation translation (should we go to the mall?) : {gu_sentence}\")\n",
    "            print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283d866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c0903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
